@startuml Everyric Sequence Diagram
!theme plain
title Everyric - 음성 자막 생성 시퀀스 다이어그램 (v2.0 - 설정 파일 기반)

actor User as U
participant "CLI (main)" as CLI
participant "config.settings" as CFG
participant "audio.downloader" as DL
participant "text.lyrics" as LY
participant "audio.processor" as AP
participant "align.smart_aligner" as SA
participant "text.pronunciation" as PR
participant "translation.translator" as TR
participant "output.subtitle_formatter" as SF
participant "audio.separator" as SEP
participant "audio.whisperx_processor" as WX
participant "utils.output_manager" as OM
participant "utils.word_level_formatter" as WLF
participant "visualization.diagnostics" as VIZ
participant "YouTube/Local File" as YT
participant "Whisper Model" as WM
participant "OpenAI GPT" as GPT
participant "File System" as FS

== Configuration Phase ==
U -> CLI: python cli.py [--config config.json]
activate CLI

CLI -> CLI: setup_argparse()
CLI -> CFG: load_config(config_path)
activate CFG
CFG -> FS: read config.json
FS --> CFG: config_data
CFG -> CFG: apply settings to EveryricConfig
CFG --> CLI: config object
deactivate CFG

CLI -> CLI: validate required settings
CLI -> U: [Everyric] 설정 파일: config.json
CLI -> U: [Everyric] 오디오: URL/파일
CLI -> U: [Everyric] 가사: lyrics.txt
CLI -> U: [Everyric] 발음표기: 포함/미포함
CLI -> U: [Everyric] 번역: 포함/미포함

== Audio Download Phase ==
CLI -> DL: download_audio(config.audio, config.output)
activate DL
DL -> FS: makedirs(output_dir)
DL -> YT: YoutubeDL.extract_info(url)
activate YT
YT --> DL: video_info + audio_stream
deactivate YT
DL -> FS: save audio file
DL --> CLI: audio_file_path
deactivate DL

== Vocal Separation Phase (Optional) ==
alt config.vocal_separation == true
    CLI -> SEP: separate_vocals(audio_path, output_dir, engine, model)
    activate SEP
    SEP -> FS: write audio_vocals.wav + audio_no_vocals.wav
    SEP --> CLI: vocals_path, accomp_path
    deactivate SEP
    CLI -> CLI: audio_path = vocals_path
else
    CLI -> CLI: skip
end

== ASR Phase ==
alt config.asr_engine == "whisperx"
    CLI -> WX: transcribe_audio_with_whisperx(audio_path, device, model, ...)
    activate WX
    WX --> CLI: segments (may include words[])
    CLI -> WX: detect_silence_segments(audio_path, vad_min_silence_len, vad_silence_thresh)
    WX --> CLI: silence_ranges[]
    CLI -> WX: apply_silence_based_timing_correction(segments, silence_ranges)
    WX --> CLI: segments_corrected
    deactivate WX

    CLI -> OM: save_asr_output(segments_corrected, output_dir, 'whisperx')
    OM -> FS: asr_whisperx_segments.json/txt
    OM --> CLI: saved_paths

    CLI -> SF: save_aligned_subtitles(segments_corrected, output_dir, filename_prefix=config.asr_debug_prefix)
    activate SF
    SF -> FS: whisperx_only_subtitles.srt + whisperx_only_aligned_subtitles.json
    deactivate SF

    CLI -> WLF: save_word_level_subtitles(segments, output_dir, filename_prefix=config.whisperx_precise_prefix_prevad)
    activate WLF
    WLF -> FS: whisperx_precise_prevad_subtitles.srt/json
    WLF --> CLI: files_pre
    deactivate WLF

    CLI -> WLF: save_word_level_subtitles(segments_corrected, output_dir, filename_prefix=config.whisperx_precise_prefix_postvad)
    activate WLF
    WLF -> FS: whisperx_precise_postvad_subtitles.srt/json
    WLF --> CLI: files_post
    deactivate WLF
else
    CLI -> AP: transcribe_audio(audio_path, device, model)
    activate AP
    AP -> WM: load + transcribe
    activate WM
    WM --> AP: segments with timing
    deactivate WM
    AP --> CLI: transcription_segments
    deactivate AP

    CLI -> OM: save_asr_output(transcription_segments, output_dir, 'whisper')
    OM -> FS: asr_whisper_segments.json/txt
    OM --> CLI: saved_paths
end

== Lyrics Processing Phase ==
alt config.lyrics exists
    CLI -> LY: parse_lyrics(config.lyrics)
    activate LY
    LY -> FS: open(lyrics_file, 'r')
    FS --> LY: file_content
    LY -> LY: split lines + clean text
    LY --> CLI: lyrics_lines[]
    deactivate LY

    == Smart Alignment Phase ==
    CLI -> SA: SmartAligner(max_segment_length=config.max_length, alignment_config)
    activate SA
    CLI -> SA: align_lyrics_with_timing(lyrics_lines, segments or segments_corrected)
    SA -> SA: calculate similarity scores
    SA -> SA: split long segments if needed
    SA --> CLI: aligned_subtitles[]
    deactivate SA

    == Translation Phase (Optional) ==
    alt config.translate == true
        CLI -> TR: translate_lyrics_with_gpt(lyrics_lines, config)
        activate TR
        TR -> GPT: ChatCompletion.create(messages)
        activate GPT
        GPT --> TR: translated_text
        deactivate GPT
        TR -> TR: parse translation response
        TR --> CLI: translated_lyrics[]
        deactivate TR
    end

    == Pronunciation Phase (Optional) ==
    alt config.pronunciation == true
        CLI -> PR: convert_pronunciation(text, config.source_lang)
        activate PR
        PR -> PR: detect language + apply mapping
        PR --> CLI: pronunciation_text
        deactivate PR
    end

    == Subtitle Generation Phase ==
    CLI -> SF: save_aligned_subtitles(aligned_subtitles, output_dir, include_pronunciation, source_lang, translated_lyrics)
    activate SF
    SF -> FS: write subtitles.srt
    SF -> FS: write aligned_subtitles.json
    SF --> CLI: output_files[]
    deactivate SF

else config.lyrics is empty
    == Transcription Only Mode ==
    CLI -> SF: save_transcription_text(segments, config.output)
    activate SF
    SF -> FS: write transcription.txt
    SF --> CLI: output_file
    deactivate SF
end

== Visualization Phase (Optional) ==
alt config.enable_visual_diagnostics == true
    CLI -> VIZ: render_diagnostics_image(output_dir, original_audio_path, accompaniment_audio_path, vocals_audio_path, aligned_subtitles, segments or segments_corrected, lyrics_lines, silence_ranges)
    activate VIZ
    VIZ -> FS: write diagnostics.png
    VIZ --> CLI: diagnostics_path
    deactivate VIZ
end

== Completion Phase ==
CLI -> U: ✅ 모든 처리 완료
CLI -> U: 💾 생성된 파일들: output_files
deactivate CLI

@enduml